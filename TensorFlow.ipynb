{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sh-Dulguun/Dulguun.github.io/blob/master/TensorFlow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Preprocessings**"
      ],
      "metadata": {
        "id": "tLVD94QBnaxM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.compat.v1.disable_eager_execution()\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from keras.datasets import mnist\n",
        "from matplotlib import pyplot as plt"
      ],
      "metadata": {
        "id": "eGfRKVWznfNQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **[Question  1] Looking back on the scratch**"
      ],
      "metadata": {
        "id": "OQYCkZt2mIjD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_IqzMZ_Kwxud"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"Iris.csv\")\n",
        "df = df[(df[\"Species\"] == \"Iris-versicolor\") | (df[\"Species\"] == \"Iris-virginica\")]\n",
        "y = df[\"Species\"]\n",
        "X = df.loc[:, [\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"]]\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "y[y == \"Iris-versicolor\"] = 0\n",
        "y[y == \"Iris-virginica\"] = 1\n",
        "y = y.astype(np.int64)[:, np.newaxis]\n",
        "X_train, X_test, y_train,y_test = train_test_split(X,y, test_size = 0.2, random_state = 0)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **[Problem 2] Consider the correspondence between Scratch and TensorFlow**"
      ],
      "metadata": {
        "id": "Y7neRva7seH1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GetMiniBatch:\n",
        "    \"\"\"\n",
        "    ミニバッチを取得するイテレータ\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    X : 次の形のndarray, shape (n_samples, n_features)\n",
        "      訓練データ\n",
        "    y : 次の形のndarray, shape (n_samples, 1)\n",
        "      正解値\n",
        "    batch_size : int\n",
        "      バッチサイズ\n",
        "    seed : int\n",
        "      NumPyの乱数のシード\n",
        "    \"\"\"\n",
        "    def __init__(self, X, y, batch_size = 10, seed=0):\n",
        "        self.batch_size = batch_size\n",
        "        np.random.seed(seed)\n",
        "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
        "        self.X = X[shuffle_index]\n",
        "        self.y = y[shuffle_index]\n",
        "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
        "    def __len__(self):\n",
        "        return self._stop\n",
        "    def __getitem__(self,item):\n",
        "        p0 = item*self.batch_size\n",
        "        p1 = item*self.batch_size + self.batch_size\n",
        "        return self.X[p0:p1], self.y[p0:p1]\n",
        "    def __iter__(self):\n",
        "        self._counter = 0\n",
        "        return self\n",
        "    def __next__(self):\n",
        "        if self._counter >= self._stop:\n",
        "            raise StopIteration()\n",
        "        p0 = self._counter*self.batch_size\n",
        "        p1 = self._counter*self.batch_size + self.batch_size\n",
        "        self._counter += 1\n",
        "        return self.X[p0:p1], self.y[p0:p1]\n",
        "\n",
        "# Configuring Hyperparameters\n",
        "learning_rate = 0.001\n",
        "batch_size = 10\n",
        "num_epochs = 100\n",
        "\n",
        "n_hidden1 = 50\n",
        "n_hidden2 = 100\n",
        "n_input = X_train.shape[1]\n",
        "n_samples = X_train.shape[0]\n",
        "n_classes = 1\n",
        "\n",
        "# Determine the shape of the arguments to be passed to the calculation graph\n",
        "X = tf.placeholder(\"float\", [None, n_input])\n",
        "Y = tf.placeholder(\"float\", [None, n_classes])\n",
        "# trainのミニバッチイテレータ\n",
        "get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "IIgHzQ-w4TFg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def example_net(x):\n",
        "    \"\"\"\n",
        "    単純な3層ニューラルネットワーク\n",
        "    \"\"\"\n",
        "    tf.random.set_random_seed(0)\n",
        "    # 重みとバイアスの宣言\n",
        "    weights = {\n",
        "        'w1': tf.Variable(tf.random_normal([n_input, n_hidden1])),\n",
        "        'w2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2])),\n",
        "        'w3': tf.Variable(tf.random_normal([n_hidden2, n_classes]))\n",
        "    }\n",
        "    biases = {\n",
        "        'b1': tf.Variable(tf.random_normal([n_hidden1])),\n",
        "        'b2': tf.Variable(tf.random_normal([n_hidden2])),\n",
        "        'b3': tf.Variable(tf.random_normal([n_classes]))\n",
        "    }\n",
        "\n",
        "    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n",
        "    layer_1 = tf.nn.relu(layer_1)\n",
        "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
        "    layer_2 = tf.nn.relu(layer_2)\n",
        "    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3'] # tf.addと+は等価である\n",
        "    return layer_output\n",
        "\n",
        "# Reading the network structure\n",
        "logits = example_net(X)\n",
        "\n",
        "# Objective function\n",
        "loss_op = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=Y, logits=logits))\n",
        "# Optimization techniques\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "train_op = optimizer.minimize(loss_op)\n",
        "\n",
        "# Estimated results\n",
        "correct_pred = tf.equal(tf.sign(Y - 0.5), tf.sign(tf.sigmoid(logits) - 0.5))\n",
        "# Index value calculation\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "\n",
        "# Initializing a variable\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "\n",
        "# Running Calculation Graphs\n",
        "with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    for epoch in range(num_epochs):\n",
        "        # Loop every epoch\n",
        "        total_batch = np.ceil(X_train.shape[0]/batch_size).astype(np.int64)\n",
        "        total_loss = 0\n",
        "        total_acc = 0\n",
        "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
        "            # Loop per mini-batch\n",
        "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
        "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
        "            total_loss += loss\n",
        "        total_loss /= n_samples\n",
        "        val_loss, acc = sess.run([loss_op, accuracy], feed_dict={X: X_val, Y: y_val})\n",
        "        print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}, acc : {:.3f}\".format(epoch, total_loss, val_loss, acc))\n",
        "    test_acc = sess.run(accuracy, feed_dict={X: X_test, Y: y_test})\n",
        "    print(\"test_acc : {:.3f}\".format(test_acc))\n",
        "\n",
        "\n",
        "print(\"It is much faster than implemented CNN from scratch. It's also very easy to use \\\n",
        "\\nFirst, weights and biases are initialized and then layers are defined.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dg3TmPlJsnPm",
        "outputId": "27623438-74a5-4958-e288-3f762be46966"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, loss : 47.9514, val_loss : 284.0979, acc : 0.625\n",
            "Epoch 1, loss : 43.0378, val_loss : 254.2579, acc : 0.625\n",
            "Epoch 2, loss : 38.3406, val_loss : 225.3243, acc : 0.625\n",
            "Epoch 3, loss : 33.7537, val_loss : 196.6855, acc : 0.625\n",
            "Epoch 4, loss : 29.2011, val_loss : 167.4652, acc : 0.625\n",
            "Epoch 5, loss : 24.3960, val_loss : 136.3264, acc : 0.625\n",
            "Epoch 6, loss : 19.2588, val_loss : 103.7501, acc : 0.625\n",
            "Epoch 7, loss : 14.0444, val_loss : 70.9598, acc : 0.625\n",
            "Epoch 8, loss : 8.8916, val_loss : 38.7318, acc : 0.625\n",
            "Epoch 9, loss : 4.1360, val_loss : 18.8356, acc : 0.125\n",
            "Epoch 10, loss : 2.5106, val_loss : 30.8900, acc : 0.375\n",
            "Epoch 11, loss : 2.6513, val_loss : 25.4411, acc : 0.375\n",
            "Epoch 12, loss : 1.7809, val_loss : 9.8705, acc : 0.438\n",
            "Epoch 13, loss : 1.3557, val_loss : 6.5804, acc : 0.438\n",
            "Epoch 14, loss : 1.0566, val_loss : 7.7044, acc : 0.438\n",
            "Epoch 15, loss : 0.9485, val_loss : 7.1685, acc : 0.438\n",
            "Epoch 16, loss : 0.7923, val_loss : 4.1130, acc : 0.500\n",
            "Epoch 17, loss : 0.6604, val_loss : 4.2018, acc : 0.562\n",
            "Epoch 18, loss : 0.5863, val_loss : 3.9233, acc : 0.562\n",
            "Epoch 19, loss : 0.5066, val_loss : 2.8078, acc : 0.625\n",
            "Epoch 20, loss : 0.4323, val_loss : 2.6520, acc : 0.625\n",
            "Epoch 21, loss : 0.3754, val_loss : 2.2355, acc : 0.625\n",
            "Epoch 22, loss : 0.3151, val_loss : 1.6709, acc : 0.688\n",
            "Epoch 23, loss : 0.2637, val_loss : 1.3452, acc : 0.688\n",
            "Epoch 24, loss : 0.2170, val_loss : 0.9584, acc : 0.750\n",
            "Epoch 25, loss : 0.1749, val_loss : 0.7231, acc : 0.812\n",
            "Epoch 26, loss : 0.1391, val_loss : 0.5406, acc : 0.812\n",
            "Epoch 27, loss : 0.1111, val_loss : 0.3738, acc : 0.812\n",
            "Epoch 28, loss : 0.0923, val_loss : 0.2388, acc : 0.875\n",
            "Epoch 29, loss : 0.0778, val_loss : 0.1460, acc : 0.938\n",
            "Epoch 30, loss : 0.0676, val_loss : 0.0822, acc : 0.938\n",
            "Epoch 31, loss : 0.0602, val_loss : 0.0430, acc : 1.000\n",
            "Epoch 32, loss : 0.0544, val_loss : 0.0261, acc : 1.000\n",
            "Epoch 33, loss : 0.0498, val_loss : 0.0182, acc : 1.000\n",
            "Epoch 34, loss : 0.0456, val_loss : 0.0130, acc : 1.000\n",
            "Epoch 35, loss : 0.0417, val_loss : 0.0091, acc : 1.000\n",
            "Epoch 36, loss : 0.0377, val_loss : 0.0067, acc : 1.000\n",
            "Epoch 37, loss : 0.0340, val_loss : 0.0056, acc : 1.000\n",
            "Epoch 38, loss : 0.0307, val_loss : 0.0053, acc : 1.000\n",
            "Epoch 39, loss : 0.0275, val_loss : 0.0057, acc : 1.000\n",
            "Epoch 40, loss : 0.0246, val_loss : 0.0067, acc : 1.000\n",
            "Epoch 41, loss : 0.0219, val_loss : 0.0082, acc : 1.000\n",
            "Epoch 42, loss : 0.0195, val_loss : 0.0100, acc : 1.000\n",
            "Epoch 43, loss : 0.0174, val_loss : 0.0120, acc : 1.000\n",
            "Epoch 44, loss : 0.0155, val_loss : 0.0142, acc : 1.000\n",
            "Epoch 45, loss : 0.0136, val_loss : 0.0170, acc : 1.000\n",
            "Epoch 46, loss : 0.0118, val_loss : 0.0204, acc : 1.000\n",
            "Epoch 47, loss : 0.0100, val_loss : 0.0246, acc : 1.000\n",
            "Epoch 48, loss : 0.0085, val_loss : 0.0299, acc : 1.000\n",
            "Epoch 49, loss : 0.0072, val_loss : 0.0353, acc : 1.000\n",
            "Epoch 50, loss : 0.0062, val_loss : 0.0398, acc : 1.000\n",
            "Epoch 51, loss : 0.0054, val_loss : 0.0430, acc : 1.000\n",
            "Epoch 52, loss : 0.0049, val_loss : 0.0449, acc : 1.000\n",
            "Epoch 53, loss : 0.0045, val_loss : 0.0451, acc : 0.938\n",
            "Epoch 54, loss : 0.0041, val_loss : 0.0450, acc : 0.938\n",
            "Epoch 55, loss : 0.0039, val_loss : 0.0443, acc : 1.000\n",
            "Epoch 56, loss : 0.0036, val_loss : 0.0431, acc : 1.000\n",
            "Epoch 57, loss : 0.0035, val_loss : 0.0420, acc : 1.000\n",
            "Epoch 58, loss : 0.0033, val_loss : 0.0409, acc : 1.000\n",
            "Epoch 59, loss : 0.0032, val_loss : 0.0399, acc : 1.000\n",
            "Epoch 60, loss : 0.0031, val_loss : 0.0391, acc : 1.000\n",
            "Epoch 61, loss : 0.0030, val_loss : 0.0385, acc : 1.000\n",
            "Epoch 62, loss : 0.0029, val_loss : 0.0379, acc : 1.000\n",
            "Epoch 63, loss : 0.0028, val_loss : 0.0380, acc : 1.000\n",
            "Epoch 64, loss : 0.0028, val_loss : 0.0383, acc : 1.000\n",
            "Epoch 65, loss : 0.0027, val_loss : 0.0388, acc : 1.000\n",
            "Epoch 66, loss : 0.0026, val_loss : 0.0393, acc : 1.000\n",
            "Epoch 67, loss : 0.0026, val_loss : 0.0398, acc : 1.000\n",
            "Epoch 68, loss : 0.0025, val_loss : 0.0402, acc : 1.000\n",
            "Epoch 69, loss : 0.0025, val_loss : 0.0406, acc : 1.000\n",
            "Epoch 70, loss : 0.0024, val_loss : 0.0410, acc : 1.000\n",
            "Epoch 71, loss : 0.0024, val_loss : 0.0414, acc : 1.000\n",
            "Epoch 72, loss : 0.0024, val_loss : 0.0417, acc : 1.000\n",
            "Epoch 73, loss : 0.0023, val_loss : 0.0420, acc : 1.000\n",
            "Epoch 74, loss : 0.0023, val_loss : 0.0423, acc : 1.000\n",
            "Epoch 75, loss : 0.0022, val_loss : 0.0425, acc : 1.000\n",
            "Epoch 76, loss : 0.0022, val_loss : 0.0428, acc : 1.000\n",
            "Epoch 77, loss : 0.0022, val_loss : 0.0430, acc : 1.000\n",
            "Epoch 78, loss : 0.0021, val_loss : 0.0431, acc : 1.000\n",
            "Epoch 79, loss : 0.0021, val_loss : 0.0433, acc : 1.000\n",
            "Epoch 80, loss : 0.0021, val_loss : 0.0434, acc : 1.000\n",
            "Epoch 81, loss : 0.0020, val_loss : 0.0435, acc : 1.000\n",
            "Epoch 82, loss : 0.0020, val_loss : 0.0436, acc : 1.000\n",
            "Epoch 83, loss : 0.0020, val_loss : 0.0437, acc : 1.000\n",
            "Epoch 84, loss : 0.0019, val_loss : 0.0438, acc : 1.000\n",
            "Epoch 85, loss : 0.0019, val_loss : 0.0439, acc : 0.938\n",
            "Epoch 86, loss : 0.0019, val_loss : 0.0439, acc : 0.938\n",
            "Epoch 87, loss : 0.0019, val_loss : 0.0439, acc : 0.938\n",
            "Epoch 88, loss : 0.0018, val_loss : 0.0440, acc : 0.938\n",
            "Epoch 89, loss : 0.0018, val_loss : 0.0440, acc : 0.938\n",
            "Epoch 90, loss : 0.0018, val_loss : 0.0440, acc : 0.938\n",
            "Epoch 91, loss : 0.0018, val_loss : 0.0440, acc : 0.938\n",
            "Epoch 92, loss : 0.0017, val_loss : 0.0439, acc : 0.938\n",
            "Epoch 93, loss : 0.0017, val_loss : 0.0439, acc : 0.938\n",
            "Epoch 94, loss : 0.0017, val_loss : 0.0439, acc : 0.938\n",
            "Epoch 95, loss : 0.0017, val_loss : 0.0438, acc : 0.938\n",
            "Epoch 96, loss : 0.0017, val_loss : 0.0438, acc : 0.938\n",
            "Epoch 97, loss : 0.0016, val_loss : 0.0437, acc : 1.000\n",
            "Epoch 98, loss : 0.0016, val_loss : 0.0436, acc : 1.000\n",
            "Epoch 99, loss : 0.0016, val_loss : 0.0436, acc : 1.000\n",
            "test_acc : 0.900\n",
            "It is much faster than implemented CNN from scratch. It's also very easy to use \n",
            "First, weights and biases are initialized and then layers are defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **[Problem 3] Create a model of Iris using all three types of objective variables**"
      ],
      "metadata": {
        "id": "jqYq9SBR-LOQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"Iris.csv\")\n",
        "df = df[(df[\"Species\"] == \"Iris-versicolor\") | (df[\"Species\"] == \"Iris-virginica\") | (df[\"Species\"]==\"Iris-setosa\")]\n",
        "y = df[\"Species\"]\n",
        "X = df.loc[:, [\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"]]\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "y[y == \"Iris-versicolor\"] = 0\n",
        "y[y == \"Iris-virginica\"] = 1\n",
        "y[y == \"Iris-setosa\"] = 2\n",
        "\n",
        "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
        "y = enc.fit_transform(y[:,np.newaxis])\n",
        "\n",
        "X_train, X_test, y_train,y_test = train_test_split(X,y, test_size = 0.2, random_state = 0)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
        "\n",
        "mmsc = MinMaxScaler()\n",
        "X_train = mmsc.fit_transform(X_train)\n",
        "X_test = mmsc.transform(X_test)\n",
        "X_val = mmsc.transform(X_val)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ks-dfBOy7rc8",
        "outputId": "75c54dce-ddb8-4d8f-fd7d-f7a2b5b294cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 0.001\n",
        "batch_size = 10\n",
        "num_epochs = 100\n",
        "n_hidden1 = 50\n",
        "n_hidden2 = 100\n",
        "n_input = X_train.shape[1]\n",
        "n_samples = X_train.shape[0]\n",
        "\n",
        "n_classes = 3\n",
        "X = tf.placeholder(\"float\", [None, n_input])\n",
        "Y = tf.placeholder(\"float\", [None, n_classes])\n",
        "\n",
        "get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size)\n",
        "\n",
        "logits = example_net(X)\n",
        "loss_op = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=Y, logits=logits))\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "train_op = optimizer.minimize(loss_op)\n",
        "correct_pred = tf.equal(tf.sign(Y - 0.5), tf.sign(tf.sigmoid(logits) - 0.5))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    for epoch in range(num_epochs):\n",
        "        # Loop every epoch\n",
        "        total_batch = np.ceil(X_train.shape[0]/batch_size).astype(np.int64)\n",
        "        total_loss = 0\n",
        "        total_acc = 0\n",
        "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
        "            # Loop per mini-batch\n",
        "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
        "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
        "            total_loss += loss\n",
        "        total_loss /= n_samples\n",
        "        val_loss, acc = sess.run([loss_op, accuracy], feed_dict={X: X_val, Y: y_val})\n",
        "        print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}, acc : {:.3f}\".format(epoch, total_loss, val_loss, acc))\n",
        "    test_acc = sess.run(accuracy, feed_dict={X: X_test, Y: y_test})\n",
        "    print(\"test_acc : {:.3f}\".format(test_acc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-4hrZsjFBSVv",
        "outputId": "3022021e-23ae-46e6-d9c9-4a165834217d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-48-30d56e557bd2>:22: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, loss : 1.3033, val_loss : 9.7414, acc : 0.403\n",
            "Epoch 1, loss : 0.7933, val_loss : 5.0665, acc : 0.486\n",
            "Epoch 2, loss : 0.4429, val_loss : 2.9194, acc : 0.611\n",
            "Epoch 3, loss : 0.2443, val_loss : 1.3513, acc : 0.708\n",
            "Epoch 4, loss : 0.0955, val_loss : 0.5434, acc : 0.875\n",
            "Epoch 5, loss : 0.0501, val_loss : 0.5281, acc : 0.875\n",
            "Epoch 6, loss : 0.0363, val_loss : 0.3961, acc : 0.917\n",
            "Epoch 7, loss : 0.0279, val_loss : 0.3500, acc : 0.931\n",
            "Epoch 8, loss : 0.0247, val_loss : 0.3298, acc : 0.917\n",
            "Epoch 9, loss : 0.0223, val_loss : 0.3184, acc : 0.917\n",
            "Epoch 10, loss : 0.0208, val_loss : 0.3011, acc : 0.917\n",
            "Epoch 11, loss : 0.0195, val_loss : 0.2863, acc : 0.931\n",
            "Epoch 12, loss : 0.0183, val_loss : 0.2731, acc : 0.931\n",
            "Epoch 13, loss : 0.0172, val_loss : 0.2599, acc : 0.931\n",
            "Epoch 14, loss : 0.0162, val_loss : 0.2479, acc : 0.931\n",
            "Epoch 15, loss : 0.0153, val_loss : 0.2375, acc : 0.931\n",
            "Epoch 16, loss : 0.0145, val_loss : 0.2282, acc : 0.931\n",
            "Epoch 17, loss : 0.0137, val_loss : 0.2196, acc : 0.931\n",
            "Epoch 18, loss : 0.0131, val_loss : 0.2116, acc : 0.944\n",
            "Epoch 19, loss : 0.0124, val_loss : 0.2039, acc : 0.944\n",
            "Epoch 20, loss : 0.0118, val_loss : 0.1968, acc : 0.944\n",
            "Epoch 21, loss : 0.0113, val_loss : 0.1905, acc : 0.944\n",
            "Epoch 22, loss : 0.0108, val_loss : 0.1849, acc : 0.944\n",
            "Epoch 23, loss : 0.0103, val_loss : 0.1799, acc : 0.944\n",
            "Epoch 24, loss : 0.0098, val_loss : 0.1749, acc : 0.944\n",
            "Epoch 25, loss : 0.0094, val_loss : 0.1704, acc : 0.944\n",
            "Epoch 26, loss : 0.0090, val_loss : 0.1662, acc : 0.944\n",
            "Epoch 27, loss : 0.0086, val_loss : 0.1624, acc : 0.944\n",
            "Epoch 28, loss : 0.0083, val_loss : 0.1588, acc : 0.944\n",
            "Epoch 29, loss : 0.0079, val_loss : 0.1554, acc : 0.944\n",
            "Epoch 30, loss : 0.0076, val_loss : 0.1525, acc : 0.944\n",
            "Epoch 31, loss : 0.0073, val_loss : 0.1491, acc : 0.944\n",
            "Epoch 32, loss : 0.0070, val_loss : 0.1455, acc : 0.958\n",
            "Epoch 33, loss : 0.0067, val_loss : 0.1420, acc : 0.958\n",
            "Epoch 34, loss : 0.0065, val_loss : 0.1386, acc : 0.958\n",
            "Epoch 35, loss : 0.0063, val_loss : 0.1351, acc : 0.958\n",
            "Epoch 36, loss : 0.0060, val_loss : 0.1316, acc : 0.958\n",
            "Epoch 37, loss : 0.0058, val_loss : 0.1284, acc : 0.958\n",
            "Epoch 38, loss : 0.0056, val_loss : 0.1254, acc : 0.958\n",
            "Epoch 39, loss : 0.0054, val_loss : 0.1226, acc : 0.958\n",
            "Epoch 40, loss : 0.0053, val_loss : 0.1199, acc : 0.958\n",
            "Epoch 41, loss : 0.0051, val_loss : 0.1175, acc : 0.958\n",
            "Epoch 42, loss : 0.0049, val_loss : 0.1154, acc : 0.958\n",
            "Epoch 43, loss : 0.0048, val_loss : 0.1136, acc : 0.958\n",
            "Epoch 44, loss : 0.0047, val_loss : 0.1119, acc : 0.958\n",
            "Epoch 45, loss : 0.0046, val_loss : 0.1109, acc : 0.972\n",
            "Epoch 46, loss : 0.0045, val_loss : 0.1098, acc : 0.972\n",
            "Epoch 47, loss : 0.0044, val_loss : 0.1086, acc : 0.972\n",
            "Epoch 48, loss : 0.0043, val_loss : 0.1078, acc : 0.972\n",
            "Epoch 49, loss : 0.0042, val_loss : 0.1071, acc : 0.972\n",
            "Epoch 50, loss : 0.0041, val_loss : 0.1065, acc : 0.958\n",
            "Epoch 51, loss : 0.0040, val_loss : 0.1064, acc : 0.958\n",
            "Epoch 52, loss : 0.0040, val_loss : 0.1062, acc : 0.958\n",
            "Epoch 53, loss : 0.0039, val_loss : 0.1056, acc : 0.958\n",
            "Epoch 54, loss : 0.0038, val_loss : 0.1056, acc : 0.958\n",
            "Epoch 55, loss : 0.0038, val_loss : 0.1058, acc : 0.958\n",
            "Epoch 56, loss : 0.0037, val_loss : 0.1058, acc : 0.944\n",
            "Epoch 57, loss : 0.0036, val_loss : 0.1059, acc : 0.944\n",
            "Epoch 58, loss : 0.0036, val_loss : 0.1061, acc : 0.944\n",
            "Epoch 59, loss : 0.0035, val_loss : 0.1062, acc : 0.944\n",
            "Epoch 60, loss : 0.0035, val_loss : 0.1061, acc : 0.944\n",
            "Epoch 61, loss : 0.0034, val_loss : 0.1063, acc : 0.944\n",
            "Epoch 62, loss : 0.0034, val_loss : 0.1069, acc : 0.944\n",
            "Epoch 63, loss : 0.0033, val_loss : 0.1073, acc : 0.958\n",
            "Epoch 64, loss : 0.0033, val_loss : 0.1075, acc : 0.958\n",
            "Epoch 65, loss : 0.0032, val_loss : 0.1076, acc : 0.958\n",
            "Epoch 66, loss : 0.0032, val_loss : 0.1081, acc : 0.944\n",
            "Epoch 67, loss : 0.0031, val_loss : 0.1085, acc : 0.944\n",
            "Epoch 68, loss : 0.0031, val_loss : 0.1089, acc : 0.944\n",
            "Epoch 69, loss : 0.0031, val_loss : 0.1089, acc : 0.944\n",
            "Epoch 70, loss : 0.0030, val_loss : 0.1089, acc : 0.944\n",
            "Epoch 71, loss : 0.0030, val_loss : 0.1090, acc : 0.944\n",
            "Epoch 72, loss : 0.0030, val_loss : 0.1090, acc : 0.944\n",
            "Epoch 73, loss : 0.0029, val_loss : 0.1090, acc : 0.944\n",
            "Epoch 74, loss : 0.0029, val_loss : 0.1091, acc : 0.944\n",
            "Epoch 75, loss : 0.0029, val_loss : 0.1090, acc : 0.944\n",
            "Epoch 76, loss : 0.0029, val_loss : 0.1096, acc : 0.944\n",
            "Epoch 77, loss : 0.0028, val_loss : 0.1097, acc : 0.944\n",
            "Epoch 78, loss : 0.0028, val_loss : 0.1093, acc : 0.944\n",
            "Epoch 79, loss : 0.0028, val_loss : 0.1093, acc : 0.944\n",
            "Epoch 80, loss : 0.0028, val_loss : 0.1099, acc : 0.944\n",
            "Epoch 81, loss : 0.0027, val_loss : 0.1101, acc : 0.944\n",
            "Epoch 82, loss : 0.0027, val_loss : 0.1100, acc : 0.944\n",
            "Epoch 83, loss : 0.0027, val_loss : 0.1099, acc : 0.944\n",
            "Epoch 84, loss : 0.0027, val_loss : 0.1101, acc : 0.944\n",
            "Epoch 85, loss : 0.0026, val_loss : 0.1103, acc : 0.944\n",
            "Epoch 86, loss : 0.0026, val_loss : 0.1104, acc : 0.944\n",
            "Epoch 87, loss : 0.0026, val_loss : 0.1105, acc : 0.944\n",
            "Epoch 88, loss : 0.0026, val_loss : 0.1105, acc : 0.944\n",
            "Epoch 89, loss : 0.0026, val_loss : 0.1107, acc : 0.944\n",
            "Epoch 90, loss : 0.0025, val_loss : 0.1109, acc : 0.944\n",
            "Epoch 91, loss : 0.0025, val_loss : 0.1113, acc : 0.944\n",
            "Epoch 92, loss : 0.0025, val_loss : 0.1116, acc : 0.944\n",
            "Epoch 93, loss : 0.0025, val_loss : 0.1116, acc : 0.944\n",
            "Epoch 94, loss : 0.0025, val_loss : 0.1116, acc : 0.944\n",
            "Epoch 95, loss : 0.0024, val_loss : 0.1117, acc : 0.944\n",
            "Epoch 96, loss : 0.0024, val_loss : 0.1119, acc : 0.944\n",
            "Epoch 97, loss : 0.0024, val_loss : 0.1119, acc : 0.944\n",
            "Epoch 98, loss : 0.0024, val_loss : 0.1120, acc : 0.944\n",
            "Epoch 99, loss : 0.0024, val_loss : 0.1127, acc : 0.944\n",
            "test_acc : 0.989\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **[Question 4] Create a House Prices model**"
      ],
      "metadata": {
        "id": "v15nxX-KFKr3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"train.csv\")\n",
        "X = df[['GrLivArea', 'YearBuilt']].to_numpy()\n",
        "y = df[['SalePrice']].to_numpy()\n",
        "print(\"Xshape:\", X.shape)\n",
        "print(\"yshape:\", y.shape)\n",
        "X = np.log1p(X)\n",
        "y = np.log1p(y)\n",
        "\n",
        "print(\"Xshape:\", X.shape)\n",
        "print(\"yshape:\", y.shape)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
        "\n",
        "mmsc = MinMaxScaler()\n",
        "X_train = mmsc.fit_transform(X_train)\n",
        "X_test = mmsc.transform(X_test)\n",
        "X_val = mmsc.transform(X_val)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vjOtVn02FP8s",
        "outputId": "c5e5bb36-f407-4b69-e309-6c1f8b556e65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Xshape: (1460, 2)\n",
            "yshape: (1460, 1)\n",
            "Xshape: (1460, 2)\n",
            "yshape: (1460, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 0.001\n",
        "batch_size = 10\n",
        "num_epochs = 50\n",
        "n_hidden1 = 50\n",
        "n_hidden2 = 100\n",
        "n_input = X_train.shape[1]\n",
        "n_samples = X_train.shape[0]\n",
        "n_classes = 1\n",
        "X = tf.placeholder(\"float\", [None, n_input])\n",
        "Y = tf.placeholder(\"float\", [None, n_classes])\n",
        "\n",
        "get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size)\n",
        "\n",
        "logits = example_net(X)\n",
        "loss_op =  tf.losses.mean_squared_error(labels=Y, predictions=logits)\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "train_op = optimizer.minimize(loss_op)\n",
        "correct_pred = tf.equal(tf.sign(Y - 0.5), tf.sign(tf.sigmoid(logits) - 0.5))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    for epoch in range(num_epochs):\n",
        "        # Loop every epoch\n",
        "        total_batch = np.ceil(X_train.shape[0]/batch_size).astype(np.int64)\n",
        "        total_loss = 0\n",
        "        total_acc = 0\n",
        "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
        "            # Loop per mini-batch\n",
        "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
        "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
        "            total_loss += loss\n",
        "        total_loss /= n_samples\n",
        "        val_loss, acc = sess.run([loss_op, accuracy], feed_dict={X: X_val, Y: y_val})\n",
        "        print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}, acc : {:.3f}\".format(epoch, total_loss, val_loss, acc))\n",
        "    test_acc = sess.run(accuracy, feed_dict={X: X_test, Y: y_test})\n",
        "    print(\"test_acc : {:.3f}\".format(test_acc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AwyEtgYPF4qU",
        "outputId": "82862d5f-7cfd-45a8-b773-c2c13511c0ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-48-30d56e557bd2>:22: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, loss : 7.9568, val_loss : 2.2204, acc : 1.000\n",
            "Epoch 1, loss : 0.1623, val_loss : 0.8613, acc : 1.000\n",
            "Epoch 2, loss : 0.0791, val_loss : 0.4186, acc : 1.000\n",
            "Epoch 3, loss : 0.0450, val_loss : 0.2205, acc : 1.000\n",
            "Epoch 4, loss : 0.0275, val_loss : 0.1293, acc : 1.000\n",
            "Epoch 5, loss : 0.0172, val_loss : 0.0875, acc : 1.000\n",
            "Epoch 6, loss : 0.0130, val_loss : 0.0757, acc : 1.000\n",
            "Epoch 7, loss : 0.0112, val_loss : 0.0690, acc : 1.000\n",
            "Epoch 8, loss : 0.0102, val_loss : 0.0658, acc : 1.000\n",
            "Epoch 9, loss : 0.0095, val_loss : 0.0639, acc : 1.000\n",
            "Epoch 10, loss : 0.0089, val_loss : 0.0640, acc : 1.000\n",
            "Epoch 11, loss : 0.0085, val_loss : 0.0634, acc : 1.000\n",
            "Epoch 12, loss : 0.0082, val_loss : 0.0624, acc : 1.000\n",
            "Epoch 13, loss : 0.0079, val_loss : 0.0614, acc : 1.000\n",
            "Epoch 14, loss : 0.0077, val_loss : 0.0606, acc : 1.000\n",
            "Epoch 15, loss : 0.0075, val_loss : 0.0602, acc : 1.000\n",
            "Epoch 16, loss : 0.0073, val_loss : 0.0600, acc : 1.000\n",
            "Epoch 17, loss : 0.0072, val_loss : 0.0601, acc : 1.000\n",
            "Epoch 18, loss : 0.0070, val_loss : 0.0615, acc : 1.000\n",
            "Epoch 19, loss : 0.0069, val_loss : 0.0635, acc : 1.000\n",
            "Epoch 20, loss : 0.0068, val_loss : 0.0649, acc : 1.000\n",
            "Epoch 21, loss : 0.0067, val_loss : 0.0654, acc : 1.000\n",
            "Epoch 22, loss : 0.0066, val_loss : 0.0660, acc : 1.000\n",
            "Epoch 23, loss : 0.0066, val_loss : 0.0653, acc : 1.000\n",
            "Epoch 24, loss : 0.0066, val_loss : 0.0662, acc : 1.000\n",
            "Epoch 25, loss : 0.0066, val_loss : 0.0678, acc : 1.000\n",
            "Epoch 26, loss : 0.0066, val_loss : 0.0681, acc : 1.000\n",
            "Epoch 27, loss : 0.0066, val_loss : 0.0673, acc : 1.000\n",
            "Epoch 28, loss : 0.0066, val_loss : 0.0636, acc : 1.000\n",
            "Epoch 29, loss : 0.0066, val_loss : 0.0593, acc : 1.000\n",
            "Epoch 30, loss : 0.0066, val_loss : 0.0557, acc : 1.000\n",
            "Epoch 31, loss : 0.0065, val_loss : 0.0534, acc : 1.000\n",
            "Epoch 32, loss : 0.0065, val_loss : 0.0537, acc : 1.000\n",
            "Epoch 33, loss : 0.0064, val_loss : 0.0556, acc : 1.000\n",
            "Epoch 34, loss : 0.0064, val_loss : 0.0582, acc : 1.000\n",
            "Epoch 35, loss : 0.0064, val_loss : 0.0611, acc : 1.000\n",
            "Epoch 36, loss : 0.0064, val_loss : 0.0666, acc : 1.000\n",
            "Epoch 37, loss : 0.0066, val_loss : 0.0697, acc : 1.000\n",
            "Epoch 38, loss : 0.0067, val_loss : 0.0794, acc : 1.000\n",
            "Epoch 39, loss : 0.0069, val_loss : 0.0944, acc : 1.000\n",
            "Epoch 40, loss : 0.0073, val_loss : 0.1102, acc : 1.000\n",
            "Epoch 41, loss : 0.0076, val_loss : 0.1312, acc : 1.000\n",
            "Epoch 42, loss : 0.0079, val_loss : 0.1489, acc : 1.000\n",
            "Epoch 43, loss : 0.0080, val_loss : 0.1644, acc : 1.000\n",
            "Epoch 44, loss : 0.0082, val_loss : 0.1692, acc : 1.000\n",
            "Epoch 45, loss : 0.0081, val_loss : 0.1597, acc : 1.000\n",
            "Epoch 46, loss : 0.0078, val_loss : 0.1407, acc : 1.000\n",
            "Epoch 47, loss : 0.0076, val_loss : 0.1221, acc : 1.000\n",
            "Epoch 48, loss : 0.0070, val_loss : 0.1089, acc : 1.000\n",
            "Epoch 49, loss : 0.0067, val_loss : 0.0954, acc : 1.000\n",
            "test_acc : 1.000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **[Problem 5] Create an MNIST model**"
      ],
      "metadata": {
        "id": "DDVZeQkXIV7v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "X_train = X_train.reshape(-1, 784)\n",
        "X_test = X_test.reshape(-1, 784)\n",
        "\n",
        "X_train = X_train.astype(np.float)\n",
        "X_test = X_test.astype(np.float)\n",
        "\n",
        "X_train /= 255\n",
        "X_test /= 255\n",
        "\n",
        "y_train = y_train.astype(np.int)[:, np.newaxis]\n",
        "y_test = y_test.astype(np.int)[:, np.newaxis]\n",
        "\n",
        "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
        "y_train_one_hot = enc.fit_transform(y_train[:])\n",
        "y_test_one_hot = enc.fit_transform(y_test[:])\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train_one_hot, test_size=0.2)"
      ],
      "metadata": {
        "id": "6puMdwlqIdfB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 0.001\n",
        "batch_size = 10\n",
        "num_epochs = 40\n",
        "n_hidden1 = 50\n",
        "n_hidden2 = 100\n",
        "n_input = X_train.shape[1]\n",
        "n_samples = X_train.shape[0]\n",
        "n_classes = 10\n",
        "\n",
        "X = tf.placeholder(\"float\", [None, n_input])\n",
        "Y = tf.placeholder(\"float\", [None, n_classes])\n",
        "\n",
        "\n",
        "get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size)\n",
        "\n",
        "logits = example_net(X)\n",
        "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=Y, logits=logits))\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "train_op = optimizer.minimize(loss_op)\n",
        "correct_pred = tf.equal(tf.argmax(Y,1), tf.argmax(tf.nn.softmax(logits),1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    for epoch in range(num_epochs):\n",
        "        total_batch = np.ceil(X_train.shape[0]/batch_size).astype(np.int64)\n",
        "        total_loss = 0\n",
        "        total_acc = 0\n",
        "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
        "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
        "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
        "            total_loss += loss\n",
        "        total_loss /= n_samples\n",
        "        val_loss, acc = sess.run([loss_op, accuracy], feed_dict={X: X_val, Y: y_val})\n",
        "        print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}, acc : {:.3f}\".format(epoch, total_loss, val_loss, acc))\n",
        "    test_acc = sess.run(accuracy, feed_dict={X: X_test, Y: y_test_one_hot})\n",
        "    print(\"test_acc : {:.3f}\".format(test_acc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "exT8En5DJTfo",
        "outputId": "a214ab0b-21cb-4f36-d583-5c1b5918ebc1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-48-30d56e557bd2>:22: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, loss : 2.9673, val_loss : 7.4110, acc : 0.754\n",
            "Epoch 1, loss : 0.4277, val_loss : 2.5285, acc : 0.743\n",
            "Epoch 2, loss : 0.1550, val_loss : 1.2647, acc : 0.758\n",
            "Epoch 3, loss : 0.0922, val_loss : 0.9985, acc : 0.792\n",
            "Epoch 4, loss : 0.0705, val_loss : 0.8861, acc : 0.823\n",
            "Epoch 5, loss : 0.0589, val_loss : 0.8036, acc : 0.836\n",
            "Epoch 6, loss : 0.0520, val_loss : 0.7735, acc : 0.851\n",
            "Epoch 7, loss : 0.0464, val_loss : 0.6713, acc : 0.867\n",
            "Epoch 8, loss : 0.0418, val_loss : 0.6287, acc : 0.881\n",
            "Epoch 9, loss : 0.0379, val_loss : 0.6111, acc : 0.888\n",
            "Epoch 10, loss : 0.0350, val_loss : 0.6090, acc : 0.887\n",
            "Epoch 11, loss : 0.0331, val_loss : 0.5697, acc : 0.897\n",
            "Epoch 12, loss : 0.0309, val_loss : 0.5718, acc : 0.897\n",
            "Epoch 13, loss : 0.0294, val_loss : 0.5829, acc : 0.902\n",
            "Epoch 14, loss : 0.0276, val_loss : 0.5394, acc : 0.905\n",
            "Epoch 15, loss : 0.0266, val_loss : 0.5462, acc : 0.913\n",
            "Epoch 16, loss : 0.0257, val_loss : 0.5250, acc : 0.909\n",
            "Epoch 17, loss : 0.0239, val_loss : 0.5135, acc : 0.916\n",
            "Epoch 18, loss : 0.0230, val_loss : 0.5207, acc : 0.917\n",
            "Epoch 19, loss : 0.0223, val_loss : 0.5199, acc : 0.917\n",
            "Epoch 20, loss : 0.0216, val_loss : 0.5009, acc : 0.920\n",
            "Epoch 21, loss : 0.0209, val_loss : 0.5140, acc : 0.918\n",
            "Epoch 22, loss : 0.0200, val_loss : 0.5485, acc : 0.922\n",
            "Epoch 23, loss : 0.0196, val_loss : 0.5160, acc : 0.925\n",
            "Epoch 24, loss : 0.0190, val_loss : 0.5482, acc : 0.926\n",
            "Epoch 25, loss : 0.0184, val_loss : 0.5148, acc : 0.927\n",
            "Epoch 26, loss : 0.0176, val_loss : 0.5194, acc : 0.929\n",
            "Epoch 27, loss : 0.0171, val_loss : 0.5055, acc : 0.929\n",
            "Epoch 28, loss : 0.0167, val_loss : 0.5031, acc : 0.929\n",
            "Epoch 29, loss : 0.0163, val_loss : 0.5298, acc : 0.930\n",
            "Epoch 30, loss : 0.0158, val_loss : 0.5447, acc : 0.927\n",
            "Epoch 31, loss : 0.0155, val_loss : 0.5235, acc : 0.930\n",
            "Epoch 32, loss : 0.0148, val_loss : 0.5359, acc : 0.929\n",
            "Epoch 33, loss : 0.0145, val_loss : 0.5338, acc : 0.932\n",
            "Epoch 34, loss : 0.0140, val_loss : 0.5325, acc : 0.931\n",
            "Epoch 35, loss : 0.0141, val_loss : 0.5247, acc : 0.932\n",
            "Epoch 36, loss : 0.0134, val_loss : 0.5195, acc : 0.932\n",
            "Epoch 37, loss : 0.0131, val_loss : 0.5276, acc : 0.935\n",
            "Epoch 38, loss : 0.0130, val_loss : 0.5326, acc : 0.933\n",
            "Epoch 39, loss : 0.0126, val_loss : 0.5750, acc : 0.928\n",
            "test_acc : 0.927\n"
          ]
        }
      ]
    }
  ]
}